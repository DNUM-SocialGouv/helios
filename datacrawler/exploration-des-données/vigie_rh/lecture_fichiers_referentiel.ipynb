{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture fichier r√©f√©rentiel masque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine, text,inspect\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "FILE_PATH_PARQUET = '../vigie_rh/fichiers/parquet/vigierh_ref_masque.parquet'\n",
    "\n",
    "# URL BDD\n",
    "PATH_BDD = 'postgresql://helios:h3li0s@localhost:5532/helios-preprod'\n",
    "\n",
    "# Connexion √† la base\n",
    "engine = create_engine(PATH_BDD)\n",
    "\n",
    "# üì• Lire le fichier Parquet\n",
    "df = pd.read_parquet(FILE_PATH_PARQUET, engine='pyarrow')\n",
    "\n",
    "# Correspondance des colonnes (Parquet -> Table SQL)\n",
    "column_mapping = {\n",
    "    'ind_masque_code': 'code',\n",
    "    'ind_masque': 'label'\n",
    "\n",
    "}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# üîç R√©cup√©rer les colonnes existantes dans la table\n",
    "with engine.connect() as conn:\n",
    "    inspector = inspect(engine)\n",
    "    table_columns = [col[\"name\"] for col in inspector.get_columns(\"vigierh_ref_masque\")]\n",
    "\n",
    "# ‚úÇÔ∏è Garder uniquement les colonnes qui existent dans la table\n",
    "df = df[[col for col in df.columns if col in table_columns]]\n",
    "\n",
    "# Supprimer les anciennes donn√©es AVANT insertion\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DELETE FROM vigierh_ref_masque;\"))  # Supprime toutes les donn√©es\n",
    "    conn.commit()\n",
    "\n",
    "# Ins√©rer les nouvelles donn√©es en respectant la structure de la table\n",
    "df.to_sql(\n",
    "    'vigierh_ref_masque', engine, \n",
    "    if_exists='append',  # Ajoute les nouvelles donn√©es sans modifier la structure\n",
    "    index=False,  # Ne pas ins√©rer l'index Pandas\n",
    "    chunksize=1000,  # Insertion par lots pour optimiser la performance\n",
    "    method='multi'  # Regrouper les INSERT pour am√©liorer la vitesse\n",
    ")\n",
    "\n",
    "print(\" Donn√©es ins√©r√©es avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture fichier r√©f√©rentiel nature contrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine, text,inspect\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "FILE_PATH_PARQUET = '../vigie_rh/fichiers/parquet/vigierh_ref_nature_contrat.parquet'\n",
    "\n",
    "# URL BDD\n",
    "PATH_BDD = 'postgresql://helios:h3li0s@localhost:5532/helios-preprod'\n",
    "\n",
    "# Connexion √† la base\n",
    "engine = create_engine(PATH_BDD)\n",
    "\n",
    "# üì• Lire le fichier Parquet\n",
    "df = pd.read_parquet(FILE_PATH_PARQUET, engine='pyarrow')\n",
    "\n",
    "# Correspondance des colonnes (Parquet -> Table SQL)\n",
    "column_mapping = {\n",
    "    'nature_contrat_code': 'code',\n",
    "    'nature_contrat': 'label'\n",
    "\n",
    "}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# üîç R√©cup√©rer les colonnes existantes dans la table\n",
    "with engine.connect() as conn:\n",
    "    inspector = inspect(engine)\n",
    "    table_columns = [col[\"name\"] for col in inspector.get_columns(\"vigierh_ref_type_contrat\")]\n",
    "\n",
    "# ‚úÇÔ∏è Garder uniquement les colonnes qui existent dans la table\n",
    "df = df[[col for col in df.columns if col in table_columns]]\n",
    "\n",
    "# Supprimer les anciennes donn√©es AVANT insertion\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DELETE FROM vigierh_ref_type_contrat;\"))  # Supprime toutes les donn√©es\n",
    "    conn.commit()\n",
    "\n",
    "# Ins√©rer les nouvelles donn√©es en respectant la structure de la table\n",
    "df.to_sql(\n",
    "    'vigierh_ref_type_contrat', engine, \n",
    "    if_exists='append',  # Ajoute les nouvelles donn√©es sans modifier la structure\n",
    "    index=False,  # Ne pas ins√©rer l'index Pandas\n",
    "    chunksize=1000,  # Insertion par lots pour optimiser la performance\n",
    "    method='multi'  # Regrouper les INSERT pour am√©liorer la vitesse\n",
    ")\n",
    "\n",
    "print(\" Donn√©es ins√©r√©es avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture fichier r√©f√©rentiel profession1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "FILE_PATH_PARQUET = '../vigie_rh/fichiers/parquet/vigierh_ref_profession1.parquet'\n",
    "\n",
    "# URL BDD\n",
    "PATH_BDD = 'postgresql://helios:h3li0s@localhost:5532/helios-preprod'\n",
    "\n",
    "# Connexion √† la base\n",
    "engine = create_engine(PATH_BDD)\n",
    "\n",
    "# üì• Lire le fichier Parquet\n",
    "df = pd.read_parquet(FILE_PATH_PARQUET, engine='pyarrow')\n",
    "\n",
    "# Correspondance des colonnes (Parquet -> Table SQL)\n",
    "column_mapping = {\n",
    "    'profession1_code': 'code',\n",
    "    'profession1': 'label'\n",
    "\n",
    "}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# üîç R√©cup√©rer les colonnes existantes dans la table\n",
    "with engine.connect() as conn:\n",
    "    inspector = inspect(engine)\n",
    "    table_columns = [col[\"name\"] for col in inspector.get_columns(\"vigierh_ref_profession_filiere\")]\n",
    "\n",
    "# ‚úÇÔ∏è Garder uniquement les colonnes qui existent dans la table\n",
    "df = df[[col for col in df.columns if col in table_columns]]\n",
    "\n",
    "# Supprimer les anciennes donn√©es AVANT insertion\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DELETE FROM vigierh_ref_profession_filiere;\"))  # Supprime toutes les donn√©es\n",
    "    conn.commit()\n",
    "\n",
    "# Ins√©rer les nouvelles donn√©es en respectant la structure de la table\n",
    "df.to_sql(\n",
    "    'vigierh_ref_profession_filiere', engine, \n",
    "    if_exists='append',  # Ajoute les nouvelles donn√©es sans modifier la structure\n",
    "    index=False,  # Ne pas ins√©rer l'index Pandas\n",
    "    chunksize=1000,  # Insertion par lots pour optimiser la performance\n",
    "    method='multi'  # Regrouper les INSERT pour am√©liorer la vitesse\n",
    ")\n",
    "\n",
    "print(\" Donn√©es ins√©r√©es avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture fichier r√©f√©rentiel profession2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "FILE_PATH_PARQUET = '../vigie_rh/fichiers/parquet/vigierh_ref_profession2.parquet'\n",
    "\n",
    "# URL BDD\n",
    "PATH_BDD = 'postgresql://helios:h3li0s@localhost:5532/helios-preprod'\n",
    "\n",
    "# Connexion √† la base\n",
    "engine = create_engine(PATH_BDD)\n",
    "\n",
    "# üì• Lire le fichier Parquet\n",
    "df = pd.read_parquet(FILE_PATH_PARQUET, engine='pyarrow')\n",
    "\n",
    "# Correspondance des colonnes (Parquet -> Table SQL)\n",
    "column_mapping = {\n",
    "    'profession2_code': 'code',\n",
    "    'profession2': 'label'\n",
    "\n",
    "}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# üîç R√©cup√©rer les colonnes existantes dans la table\n",
    "with engine.connect() as conn:\n",
    "    inspector = inspect(engine)\n",
    "    table_columns = [col[\"name\"] for col in inspector.get_columns(\"vigierh_ref_profession_groupe\")]\n",
    "\n",
    "# ‚úÇÔ∏è Garder uniquement les colonnes qui existent dans la table\n",
    "df = df[[col for col in df.columns if col in table_columns]]\n",
    "\n",
    "# Supprimer les anciennes donn√©es AVANT insertion\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DELETE FROM vigierh_ref_profession_groupe;\"))  # Supprime toutes les donn√©es\n",
    "    conn.commit()\n",
    "\n",
    "# Ins√©rer les nouvelles donn√©es en respectant la structure de la table\n",
    "df.to_sql(\n",
    "    'vigierh_ref_profession_groupe', engine, \n",
    "    if_exists='append',  # Ajoute les nouvelles donn√©es sans modifier la structure\n",
    "    index=False,  # Ne pas ins√©rer l'index Pandas\n",
    "    chunksize=1000,  # Insertion par lots pour optimiser la performance\n",
    "    method='multi'  # Regrouper les INSERT pour am√©liorer la vitesse\n",
    ")\n",
    "\n",
    "print(\" Donn√©es ins√©r√©es avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture fichier r√©f√©rentiel qualit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "FILE_PATH_PARQUET = '../vigie_rh/fichiers/parquet/vigierh_ref_qualite.parquet'\n",
    "\n",
    "# URL BDD\n",
    "PATH_BDD = 'postgresql://helios:h3li0s@localhost:5532/helios-preprod'\n",
    "\n",
    "# Connexion √† la base\n",
    "engine = create_engine(PATH_BDD)\n",
    "\n",
    "# üì• Lire le fichier Parquet\n",
    "df = pd.read_parquet(FILE_PATH_PARQUET, engine='pyarrow')\n",
    "\n",
    "# Correspondance des colonnes (Parquet -> Table SQL)\n",
    "column_mapping = {\n",
    "    'ind_qualite_code': 'code',\n",
    "    'ind_qualite': 'label'\n",
    "\n",
    "}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# üîç R√©cup√©rer les colonnes existantes dans la table\n",
    "with engine.connect() as conn:\n",
    "    inspector = inspect(engine)\n",
    "    table_columns = [col[\"name\"] for col in inspector.get_columns(\"vigierh_ref_qualite\")]\n",
    "\n",
    "# ‚úÇÔ∏è Garder uniquement les colonnes qui existent dans la table\n",
    "df = df[[col for col in df.columns if col in table_columns]]\n",
    "\n",
    "# Supprimer les anciennes donn√©es AVANT insertion\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DELETE FROM vigierh_ref_qualite;\"))  # Supprime toutes les donn√©es\n",
    "    conn.commit()\n",
    "\n",
    "# Ins√©rer les nouvelles donn√©es en respectant la structure de la table\n",
    "df.to_sql(\n",
    "    'vigierh_ref_qualite', engine, \n",
    "    if_exists='append',  # Ajoute les nouvelles donn√©es sans modifier la structure\n",
    "    index=False,  # Ne pas ins√©rer l'index Pandas\n",
    "    chunksize=1000,  # Insertion par lots pour optimiser la performance\n",
    "    method='multi'  # Regrouper les INSERT pour am√©liorer la vitesse\n",
    ")\n",
    "\n",
    "print(\" Donn√©es ins√©r√©es avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture fichier r√©f√©rentiel redressement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine, text,inspect\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "FILE_PATH_PARQUET = '../vigie_rh/fichiers/parquet/vigierh_ref_redressement.parquet'\n",
    "\n",
    "# URL BDD\n",
    "PATH_BDD = 'postgresql://helios:h3li0s@localhost:5532/helios-preprod'\n",
    "\n",
    "# Connexion √† la base\n",
    "engine = create_engine(PATH_BDD)\n",
    "\n",
    "# üì• Lire le fichier Parquet\n",
    "df = pd.read_parquet(FILE_PATH_PARQUET, engine='pyarrow')\n",
    "\n",
    "# Correspondance des colonnes (Parquet -> Table SQL)\n",
    "column_mapping = {\n",
    "    'ind_redressement_code': 'code',\n",
    "    'ind_redressement': 'label'\n",
    "\n",
    "}\n",
    "\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# üîç R√©cup√©rer les colonnes existantes dans la table\n",
    "with engine.connect() as conn:\n",
    "    inspector = inspect(engine)\n",
    "    table_columns = [col[\"name\"] for col in inspector.get_columns(\"vigierh_ref_redressement\")]\n",
    "\n",
    "# ‚úÇÔ∏è Garder uniquement les colonnes qui existent dans la table\n",
    "df = df[[col for col in df.columns if col in table_columns]]\n",
    "\n",
    "# Supprimer les anciennes donn√©es AVANT insertion\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DELETE FROM vigierh_ref_redressement;\"))  # Supprime toutes les donn√©es\n",
    "    conn.commit()\n",
    "\n",
    "# Ins√©rer les nouvelles donn√©es en respectant la structure de la table\n",
    "df.to_sql(\n",
    "    'vigierh_ref_redressement', engine, \n",
    "    if_exists='append',  # Ajoute les nouvelles donn√©es sans modifier la structure\n",
    "    index=False,  # Ne pas ins√©rer l'index Pandas\n",
    "    chunksize=1000,  # Insertion par lots pour optimiser la performance\n",
    "    method='multi'  # Regrouper les INSERT pour am√©liorer la vitesse\n",
    ")\n",
    "\n",
    "print(\" Donn√©es ins√©r√©es avec succ√®s !\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
